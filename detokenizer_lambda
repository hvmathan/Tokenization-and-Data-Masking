import json
import boto3
import base64
import os
import csv
from io import StringIO

s3 = boto3.client('s3')

# Enable or disable masking
masking_enabled = True

# -----------------------
# Helper functions
# -----------------------

def mask_value(field, value):
    if not value:
        return value

    if field.lower() == "name":
        parts = value.split(" ")
        masked_parts = [p[0] + "*" * (len(p) - 1) for p in parts]
        return " ".join(masked_parts)

    elif field.lower() == "email":
        username, domain = value.split("@")
        masked_username = username[0] + "*" * (len(username) - 2) + username[-1] if len(username) > 2 else username
        return masked_username + "@" + domain

    elif field.lower() == "phone":
        return "*" * 6 + value[-4:]

    return value

def decode_and_mask(field, value):
    try:
        decoded = base64.b64decode(value).decode()
        return mask_value(field, decoded) if masking_enabled else decoded
    except Exception as e:
        print(f"❌ Failed to decode field {field}: {e}")
        return value  # fallback to original token if decode fails

# -----------------------
# Lambda Handler
# -----------------------

def lambda_handler(event, context):
    print("Event received:", event)

    # Get bucket and key from the event
    record = event['Records'][0]
    bucket = record['s3']['bucket']['name']
    tokenized_key = record['s3']['object']['key']

    print(f"Triggered by tokenized file: {tokenized_key}")

    # Derive original file name (assumes naming pattern: <name>_tokenized.csv)
    base_filename = os.path.basename(tokenized_key).replace("_tokenized.csv", "")
    metadata_key = f"metadata/{base_filename}_pii_fields.json"
    print(f"Looking for metadata: {metadata_key}")

    try:
        metadata_obj = s3.get_object(Bucket=bucket, Key=metadata_key)
        pii_fields = json.loads(metadata_obj['Body'].read().decode('utf-8'))
    except Exception as e:
        print(f"❌ Failed to read metadata: {e}")
        return

    print(f"PII Fields: {pii_fields}")

    try:
        tokenized_obj = s3.get_object(Bucket=bucket, Key=tokenized_key)
        raw_content = tokenized_obj['Body'].read().decode('utf-8')

        # Detect delimiter
        sniffer = csv.Sniffer()
        dialect = sniffer.sniff(raw_content.splitlines()[0])
        delimiter = dialect.delimiter
        print(f"Detected delimiter: '{delimiter}'")

        reader = csv.DictReader(StringIO(raw_content), delimiter=delimiter)
        output_rows = []

        for row in reader:
            new_row = {}
            for key in reader.fieldnames:
                value = row[key]
                if key in pii_fields:
                    value = decode_and_mask(key, value)
                new_row[key] = value
            output_rows.append(new_row)

        # Prepare output CSV
        output_csv = StringIO()
        writer = csv.DictWriter(output_csv, fieldnames=reader.fieldnames, delimiter=delimiter)
        writer.writeheader()
        writer.writerows(output_rows)

        detokenized_key = f"detokenized/{base_filename}_detokenized.csv"
        s3.put_object(Bucket=bucket, Key=detokenized_key, Body=output_csv.getvalue())
        print(f"✅ Detokenized file uploaded to: {detokenized_key}")

    except Exception as e:
        print(f"❌ Failed to process tokenized file: {e}")
